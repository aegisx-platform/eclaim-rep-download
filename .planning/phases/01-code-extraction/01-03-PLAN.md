---
phase: 01-code-extraction
plan: 03
type: execute
---

<objective>
Extract REP (Representative) downloader to eclaim_core with CLI tool.

Purpose: Create the REPDownloader class that handles OP/IP/ORF file downloads from NHSO E-Claim portal.
Output: Working REPDownloader class and cli/download_rep.py CLI tool.
</objective>

<execution_context>
~/.claude/get-shit-done/workflows/execute-phase.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/phases/01-code-extraction/01-01-SUMMARY.md
@.planning/phases/01-code-extraction/01-02-SUMMARY.md

**Source file to extract:**
@eclaim_downloader_http.py

**Base class:**
Reference: ../eclaim-downloader-core/eclaim_core/downloaders/base.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Extract REPDownloader class</name>
  <files>
../eclaim-downloader-core/eclaim_core/downloaders/rep.py
../eclaim-downloader-core/eclaim_core/downloaders/__init__.py
  </files>
  <action>
Extract and refactor `eclaim_downloader_http.py` to create REPDownloader class.

Key changes:
1. Inherit from BaseDownloader
2. Implement abstract methods: download_type, login, get_download_links, download_file
3. Use eclaim_core.types for enums and dataclasses
4. Remove argparse CLI code (will be in separate CLI tool)
5. Remove auto-import functionality (pro feature)
6. Keep BeautifulSoup for HTML parsing
7. Keep retry logic and error handling
8. Add type hints throughout

**rep.py structure:**
```python
"""
REP (Representative) Downloader
Downloads OP/IP/ORF files from NHSO E-Claim portal.
"""

import os
import time
from typing import List, Optional, Dict, Any
from datetime import datetime
from bs4 import BeautifulSoup
import requests

from .base import BaseDownloader
from ..types import (
    DownloadType, FileType, Scheme,
    DownloadResult, DownloadProgress, DownloadLink
)


class REPDownloader(BaseDownloader):
    """
    Downloads REP (Representative) files from NHSO E-Claim portal.
    Supports OP, IP, and ORF file types across multiple insurance schemes.
    """

    BASE_URL = "https://eclaim.nhso.go.th"
    LOGIN_URL = f"{BASE_URL}/webComponent/mainloginAction.do"

    # Scheme URL mappings
    SCHEME_URLS = {
        Scheme.UCS: "/webComponent/ucs/repucsAction.do",
        Scheme.OFC: "/webComponent/ofc/repofcAction.do",
        Scheme.SSS: "/webComponent/sss/repsssAction.do",
        Scheme.LGO: "/webComponent/lgo/replgoAction.do",
        Scheme.NHS: "/webComponent/nhs/repnhsAction.do",
        Scheme.BKK: "/webComponent/bkk/repbkkAction.do",
        Scheme.BMT: "/webComponent/bmt/repbmtAction.do",
        Scheme.SRT: "/webComponent/srt/repsrtAction.do",
    }

    def __init__(
        self,
        month: Optional[int] = None,
        year: Optional[int] = None,  # Buddhist Era
        schemes: Optional[List[Scheme]] = None,
        **kwargs
    ):
        super().__init__(**kwargs)
        self.month = month or self._current_be_month()
        self.year = year or self._current_be_year()
        self.schemes = schemes or [Scheme.UCS]

    @property
    def download_type(self) -> DownloadType:
        return DownloadType.REP

    @staticmethod
    def _current_be_month() -> int:
        """Get current month."""
        return datetime.now().month

    @staticmethod
    def _current_be_year() -> int:
        """Get current year in Buddhist Era."""
        return datetime.now().year + 543

    def login(self, username: str, password: str) -> bool:
        """Authenticate with NHSO E-Claim portal."""
        self.session = self._create_session()

        try:
            # Get login page for cookies
            self.session.get(self.LOGIN_URL, timeout=30)

            # Submit login
            response = self.session.post(
                self.LOGIN_URL,
                data={'user': username, 'pass': password},
                timeout=30
            )

            # Check if login successful
            if 'login' in response.url.lower() and 'error' in response.text.lower():
                self._log("Login failed - invalid credentials", level='error')
                return False

            self._log("Login successful", level='success')
            return True

        except requests.RequestException as e:
            self._log(f"Login error: {e}", level='error')
            return False

    def get_download_links(self, **kwargs) -> List[DownloadLink]:
        """Get download links for configured month/year/schemes."""
        links = []

        for scheme in self.schemes:
            scheme_links = self._get_scheme_links(scheme)
            links.extend(scheme_links)
            self._log(f"Found {len(scheme_links)} files for {scheme.value.upper()}")

        return links

    def _get_scheme_links(self, scheme: Scheme) -> List[DownloadLink]:
        """Get download links for a specific scheme."""
        if scheme not in self.SCHEME_URLS:
            self._log(f"Unsupported scheme: {scheme.value}", level='warning')
            return []

        url = f"{self.BASE_URL}{self.SCHEME_URLS[scheme]}"
        params = {
            'month': str(self.month),
            'year': str(self.year),
            'method': 'validation'
        }

        try:
            response = self.session.get(url, params=params, timeout=30)
            response.raise_for_status()
            return self._parse_download_links(response.text, scheme)
        except requests.RequestException as e:
            self._log(f"Error fetching links for {scheme.value}: {e}", level='error')
            return []

    def _parse_download_links(self, html: str, scheme: Scheme) -> List[DownloadLink]:
        """Parse HTML to extract download links."""
        soup = BeautifulSoup(html, 'html.parser')
        links = []

        # Find all download links (typically in tables)
        for link in soup.find_all('a', href=True):
            href = link['href']
            if 'download' in href.lower() or 'excel' in href.lower():
                filename = self._extract_filename(href, link.text)
                if filename:
                    full_url = href if href.startswith('http') else f"{self.BASE_URL}{href}"
                    file_type = self._detect_file_type(filename)
                    links.append(DownloadLink(
                        url=full_url,
                        filename=filename,
                        file_type=file_type,
                        scheme=scheme
                    ))

        return links

    def _extract_filename(self, href: str, link_text: str) -> Optional[str]:
        """Extract filename from URL or link text."""
        # Try URL parameters
        for param in ['fn', 'filename', 'file']:
            if f'{param}=' in href:
                parts = href.split(f'{param}=')
                if len(parts) > 1:
                    filename = parts[1].split('&')[0]
                    if filename:
                        return filename

        # Try link text
        text = link_text.strip()
        if text and '.xls' in text.lower():
            return text

        return None

    def _detect_file_type(self, filename: str) -> Optional[FileType]:
        """Detect file type from filename."""
        filename_upper = filename.upper()
        if '_OP_' in filename_upper:
            return FileType.OP
        elif '_IP_' in filename_upper:
            if 'APPEAL' in filename_upper:
                if 'NHSO' in filename_upper:
                    return FileType.IP_APPEAL_NHSO
                return FileType.IP_APPEAL
            return FileType.IP
        elif '_ORF_' in filename_upper:
            return FileType.ORF
        return None

    def download_file(self, link: DownloadLink) -> DownloadResult:
        """Download a single file."""
        file_path = os.path.join(self.download_dir, link.filename)

        # Check if already downloaded
        if self.history and self.history.exists(link.filename):
            return DownloadResult(
                success=False,
                filename=link.filename,
                file_path=file_path,
                file_size=0,
                download_type=self.download_type,
                file_type=link.file_type,
                scheme=link.scheme,
                month=self.month,
                year=self.year,
                error="skipped"
            )

        # Download with retry
        max_retries = 2
        for attempt in range(max_retries + 1):
            try:
                response = self.session.get(link.url, stream=True, timeout=60)
                response.raise_for_status()

                with open(file_path, 'wb') as f:
                    for chunk in response.iter_content(chunk_size=8192):
                        f.write(chunk)

                file_size = os.path.getsize(file_path)

                if file_size < 100:
                    raise ValueError("File too small, likely error page")

                return DownloadResult(
                    success=True,
                    filename=link.filename,
                    file_path=file_path,
                    file_size=file_size,
                    download_type=self.download_type,
                    file_type=link.file_type,
                    scheme=link.scheme,
                    month=self.month,
                    year=self.year,
                    url=link.url
                )

            except Exception as e:
                if attempt < max_retries:
                    self._log(f"Retry {attempt + 1}/{max_retries}: {e}", level='warning')
                    time.sleep(2)
                else:
                    return DownloadResult(
                        success=False,
                        filename=link.filename,
                        file_path=file_path,
                        file_size=0,
                        download_type=self.download_type,
                        file_type=link.file_type,
                        scheme=link.scheme,
                        month=self.month,
                        year=self.year,
                        error=str(e)
                    )

    def run(self, username: str, password: str) -> Dict[str, Any]:
        """
        Execute full download workflow.

        Returns:
            Dict with downloaded, skipped, errors counts
        """
        if not self.login(username, password):
            return {'success': False, 'error': 'Login failed'}

        links = self.get_download_links()
        if not links:
            return {'success': True, 'downloaded': 0, 'skipped': 0, 'errors': 0}

        results = self.download_all(links)

        downloaded = sum(1 for r in results if r.success)
        skipped = sum(1 for r in results if r.error == 'skipped')
        errors = sum(1 for r in results if r.error and r.error != 'skipped')

        return {
            'success': True,
            'downloaded': downloaded,
            'skipped': skipped,
            'errors': errors,
            'total': len(results)
        }
```

Update `eclaim_core/downloaders/__init__.py`:
```python
"""Downloader implementations."""
from .base import BaseDownloader
from .rep import REPDownloader

__all__ = ["BaseDownloader", "REPDownloader"]
```
  </action>
  <verify>`cd ../eclaim-downloader-core && python -c "from eclaim_core.downloaders import REPDownloader; d = REPDownloader(); print(d.download_type)"`</verify>
  <done>REPDownloader class extracted and working</done>
</task>

<task type="auto">
  <name>Task 2: Create CLI tool for REP downloads</name>
  <files>
../eclaim-downloader-core/cli/download_rep.py
  </files>
  <action>
Create a CLI tool for REP downloads using argparse.

**cli/download_rep.py:**
```python
#!/usr/bin/env python3
"""
REP Download CLI Tool
Download OP/IP/ORF files from NHSO E-Claim portal.

Usage:
    python -m cli.download_rep --month 1 --year 2569 --schemes ucs ofc
    python -m cli.download_rep --help
"""

import argparse
import sys
from datetime import datetime

from eclaim_core.downloaders import REPDownloader
from eclaim_core.config import SettingsManager
from eclaim_core.history import HistoryManager
from eclaim_core.logging import LogStreamer
from eclaim_core.types import Scheme


def parse_args():
    """Parse command line arguments."""
    parser = argparse.ArgumentParser(
        description="Download REP files from NHSO E-Claim portal"
    )

    # Date options
    current_month = datetime.now().month
    current_year = datetime.now().year + 543  # Buddhist Era

    parser.add_argument(
        '--month', '-m',
        type=int,
        default=current_month,
        help=f"Month (1-12), default: {current_month}"
    )
    parser.add_argument(
        '--year', '-y',
        type=int,
        default=current_year,
        help=f"Year in Buddhist Era, default: {current_year}"
    )

    # Scheme options
    parser.add_argument(
        '--schemes', '-s',
        nargs='+',
        default=['ucs'],
        choices=['ucs', 'ofc', 'sss', 'lgo', 'nhs', 'bkk', 'bmt', 'srt'],
        help="Insurance schemes to download (default: ucs)"
    )

    # Credential options
    parser.add_argument(
        '--username', '-u',
        help="NHSO username (or set ECLAIM_USERNAME env var)"
    )
    parser.add_argument(
        '--password', '-p',
        help="NHSO password (or set ECLAIM_PASSWORD env var)"
    )

    # Output options
    parser.add_argument(
        '--download-dir', '-d',
        default='./downloads',
        help="Download directory (default: ./downloads)"
    )
    parser.add_argument(
        '--quiet', '-q',
        action='store_true',
        help="Suppress output"
    )

    return parser.parse_args()


def main():
    """Main entry point."""
    args = parse_args()

    # Initialize components
    settings = SettingsManager()
    history = HistoryManager()
    logger = LogStreamer() if not args.quiet else None

    # Get credentials
    username = args.username or settings.get('eclaim_username')
    password = args.password or settings.get('eclaim_password')

    if not username or not password:
        print("Error: Username and password required")
        print("Set via --username/--password or ECLAIM_USERNAME/ECLAIM_PASSWORD env vars")
        sys.exit(1)

    # Convert scheme strings to enums
    schemes = [Scheme(s) for s in args.schemes]

    # Create downloader
    downloader = REPDownloader(
        month=args.month,
        year=args.year,
        schemes=schemes,
        download_dir=args.download_dir,
        history_manager=history,
        logger=logger
    )

    # Run download
    print(f"Downloading REP files for {args.month}/{args.year}")
    print(f"Schemes: {', '.join(args.schemes)}")
    print(f"Download directory: {args.download_dir}")
    print("-" * 40)

    result = downloader.run(username, password)

    if not result.get('success'):
        print(f"Error: {result.get('error', 'Unknown error')}")
        sys.exit(1)

    print("-" * 40)
    print(f"Downloaded: {result['downloaded']}")
    print(f"Skipped: {result['skipped']}")
    print(f"Errors: {result['errors']}")
    print(f"Total: {result['total']}")

    # Save history
    history.save()


if __name__ == '__main__':
    main()
```

Make the file executable and add shebang.
  </action>
  <verify>`cd ../eclaim-downloader-core && python -m cli.download_rep --help` shows usage</verify>
  <done>CLI tool created and shows help</done>
</task>

</tasks>

<verification>
Before declaring plan complete:
- [ ] REPDownloader class imports correctly
- [ ] REPDownloader inherits from BaseDownloader
- [ ] CLI tool shows help: `python -m cli.download_rep --help`
- [ ] CLI tool accepts month/year/schemes arguments
</verification>

<success_criteria>
- REPDownloader fully extracted with all download logic
- CLI tool functional and well-documented
- Ready for STM downloader extraction
</success_criteria>

<output>
After completion, create `.planning/phases/01-code-extraction/01-03-SUMMARY.md`
</output>
